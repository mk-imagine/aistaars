{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Text Classifier\n",
    "\n",
    "### Author: Mark Kim\n",
    "\n",
    "#### NOTE: All the code in this notebook is COMPLETE and fully functioning\n",
    "\n",
    "Recall that the Na√Øve Bayes Text Classifier with Laplace Smoothing uses the\n",
    "following MLE:\n",
    "$$ \\hat{P}(w|c) = \\frac{\\operatorname{count}(w,c) + 1}{\\operatorname{count}(c) + \\lvert\n",
    "V \\rvert}. $$\n",
    "Once all the conditional probabilities are calculated, class labels are\n",
    "determined by finding the class with the highest probability, which can be\n",
    "generalized by:\n",
    "$$ C_{NB} = \\underset{c\\in C}{\\operatorname*{argmax}}\\ P(c_j)\\prod_{x\\in X}\n",
    "P(x|c) $$\n",
    "\n",
    "This project consists of a data set of 50 restaurant reviews pulled from Yelp!\n",
    "40 reviews were used for the training set and 10 were used for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training csv file into a data structure\n",
    "What data structure do you think you should use?  Try to answer this before\n",
    "seeing my approach\n",
    "\n",
    "<details>\n",
    "  <summary>Click me to see my approach</summary>\n",
    "  \n",
    "  #### List of Tuples\n",
    "  The csv file has the text (document) being used in one column and I used an\n",
    "  integer value as a flag for the class.  In the case of \"negative\" instances, I used 0\n",
    "  and for \"positive\" instances, I used 1.\n",
    "\n",
    "  Because we are dealing with different data types (a string and an integer),\n",
    "  using a tuple in this case makes sense.  Likewise, the tuple will allow you to\n",
    "  match indices with classes, which we will see is a benefit later on.\n",
    "\n",
    "  So we will build a list of these tuples to store all the raw data of our corpus.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples from csv\n",
    "def listify_csv(csv_filename):\n",
    "    with open(csv_filename) as f:\n",
    "        # create a list of each line\n",
    "        lines = f.readlines()\n",
    "        # remember to test your code as you go along (e.g. print lines to see that\n",
    "        # the operation functions as expected)\n",
    "        # print(lines)\n",
    "\n",
    "    # list comprehension in python (strip removes new line characters)\n",
    "    lst = [(txt, int(val)) for txt,val in (tuple(line.strip().split(',')) for line in lines)]\n",
    "    # the above lines are equivalent to:\n",
    "    # lst = []\n",
    "    # for line in lines:\n",
    "    #     line_array = line.strip().split(',')\n",
    "    #     line_tuple = (line_array[0], int(line_array[1]))\n",
    "    #     lst.append(line_tuple)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lst \u001b[39m=\u001b[39m listify_csv(\u001b[39m'\u001b[39;49m\u001b[39mtrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lst\n",
      "\u001b[1;32m/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# remember to test your code as you go along (e.g. print lines to see that\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# the operation functions as expected)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# print(lines)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# list comprehension in python (strip removes new line characters)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lst \u001b[39m=\u001b[39m [(txt, \u001b[39mint\u001b[39m(val)) \u001b[39mfor\u001b[39;00m txt,val \u001b[39min\u001b[39;00m (\u001b[39mtuple\u001b[39m(line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# the above lines are equivalent to:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# lst = []\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# for line in lines:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#     line_array = line.strip().split(',')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#     line_tuple = (line_array[0], int(line_array[1]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#     lst.append(line_tuple)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m lst\n",
      "\u001b[1;32m/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# remember to test your code as you go along (e.g. print lines to see that\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# the operation functions as expected)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# print(lines)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# list comprehension in python (strip removes new line characters)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lst \u001b[39m=\u001b[39m [(txt, \u001b[39mint\u001b[39m(val)) \u001b[39mfor\u001b[39;00m txt,val \u001b[39min\u001b[39;00m (\u001b[39mtuple\u001b[39m(line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# the above lines are equivalent to:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# lst = []\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# for line in lines:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#     line_array = line.strip().split(',')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#     line_tuple = (line_array[0], int(line_array[1]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#     lst.append(line_tuple)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mark/Repos/aistaars/jupyternotebooks/nb_text_classifier.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m lst\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "lst = listify_csv('train.csv')\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Maximum Likelihood Estimator (MLE) and train it\n",
    "\n",
    "Recall the MLE above.  To make the calculation of this number more readable and\n",
    "simpler to understand, we may want to break up our data into multiple pieces of\n",
    "data.  What pieces of data do you think we should store?  Again, like the above,\n",
    "try to answer this before you see my approach.\n",
    "\n",
    "<details>\n",
    "  <summary>Click me to see my approach</summary>\n",
    "  \n",
    "  #### Na√Øve Bayes Text Classifier with Laplace Smoothing MLE\n",
    "  This MLE requires three key pieces of information: the total word count of\n",
    "  each class, the count of each unique word in each class, and the total number\n",
    "  of unique words in the corpus (e.g. the number of words in the entire\n",
    "  vocabulary).\n",
    "\n",
    "  Since we will also eventually need the probability of each class, which is simply the\n",
    "  number of documents of a particular class divided by the total number of\n",
    "  documents, we might as well create another data structure that stores this\n",
    "  information as well (because we will be iterating through the list we created\n",
    "  above anyways).\n",
    "\n",
    "  I think this will be best done by creating three different dictionaries.  The\n",
    "  first will be a dictionary with a count of documents (value) in each class (key).  We can\n",
    "  then calculate the total number of documents from this dictionary, so we\n",
    "  won't need another data structure to calculate the probability of each class.\n",
    "\n",
    "  The second dictionary will contain each unique word (key) and the counts those\n",
    "  words in each class (value).  Notice that the value will need to contain\n",
    "  multiple values as well (class and word count in that class).  We can embed\n",
    "  another dictionary as the value (a dictionary of dictionaries).  This\n",
    "  dictionary will contain all the information we need to calculate all the\n",
    "  conditional probabilities.\n",
    "\n",
    "  I initially thought that we would only need the above two dictionaries, but I\n",
    "  failed to realize that I would need the count of words in\n",
    "  each class to calculate the conditional probabilities.  So I added yet another\n",
    "  dictionary to capture these counts.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize class counts.\n",
    "class_counts = {}\n",
    "\n",
    "# Initialize empty vocab count dictionary\n",
    "vocab_counts = {}\n",
    "\n",
    "# Initialize empty class word count dictionary\n",
    "class_word_counts = {}\n",
    "\n",
    "# Testing my normalization (again: test, test, test!)\n",
    "# print(lst[0][0].lower().strip('.,\\n').split())\n",
    "doc_list = []\n",
    "\n",
    "# Iterate through list to populate our dictionaries\n",
    "for k,v in lst:\n",
    "    # Populate class counts\n",
    "    if v not in class_counts.keys():\n",
    "        class_counts[v] = 1\n",
    "    else:\n",
    "        class_counts[v] += 1\n",
    "\n",
    "    # Populate word counts for each class\n",
    "    # This takes the string and splits it into a list of words while stripping punctuation\n",
    "    doc_array = k.lower().strip('.,!?\\n').split()\n",
    "    doc_list.append(doc_array)\n",
    "    # print(f'{doc_array}, {v}')\n",
    "\n",
    "    # This iterates through the list of words to count the instances of each word\n",
    "    for word in doc_array:\n",
    "        # v is the integer representing the document class\n",
    "        # The following code checks if the class exists in the class_word_count\n",
    "        # dictionary and adds a new class if it does not, and increments the\n",
    "        # count if it does\n",
    "        if v not in class_word_counts.keys():\n",
    "            class_word_counts[v] = 1\n",
    "        else:\n",
    "            class_word_counts[v] += 1\n",
    "            \n",
    "        # If the word in the list does not exist in the vocab_counts dictionary,\n",
    "        # it adds a new entry for its document class\n",
    "        if word not in vocab_counts.keys():\n",
    "            vocab_counts[word] = {v: 1}\n",
    "        else:\n",
    "            # If the word in the list does exist in the dictionary, it must check to\n",
    "            # see if the word exists in the document class.  If the document class\n",
    "            # has a count, it increments the count, otherwise it adds a\n",
    "            # new entry for the class where no count exists.\n",
    "            if v in vocab_counts[word].keys():\n",
    "                vocab_counts[word][v] += 1\n",
    "            else:\n",
    "                vocab_counts[word][v] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'crust', 'is', 'not', 'good'],\n",
       " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty'],\n",
       " ['now',\n",
       "  'i',\n",
       "  'am',\n",
       "  'getting',\n",
       "  'angry',\n",
       "  'and',\n",
       "  'i',\n",
       "  'want',\n",
       "  'my',\n",
       "  'damn',\n",
       "  'pho'],\n",
       " ['honeslty', 'it', \"didn't\", 'taste', 'that', 'fresh'],\n",
       " ['the',\n",
       "  'potatoes',\n",
       "  'were',\n",
       "  'like',\n",
       "  'rubber',\n",
       "  'and',\n",
       "  'you',\n",
       "  'could',\n",
       "  'tell',\n",
       "  'they',\n",
       "  'had',\n",
       "  'been',\n",
       "  'made',\n",
       "  'up',\n",
       "  'ahead',\n",
       "  'of',\n",
       "  'time',\n",
       "  'being',\n",
       "  'kept',\n",
       "  'under',\n",
       "  'a',\n",
       "  'warmer'],\n",
       " ['would', 'not', 'go', 'back'],\n",
       " ['the',\n",
       "  'cashier',\n",
       "  'had',\n",
       "  'no',\n",
       "  'care',\n",
       "  'what',\n",
       "  'so',\n",
       "  'ever',\n",
       "  'on',\n",
       "  'what',\n",
       "  'i',\n",
       "  'had',\n",
       "  'to',\n",
       "  'say',\n",
       "  'it',\n",
       "  'still',\n",
       "  'ended',\n",
       "  'up',\n",
       "  'being',\n",
       "  'wayyy',\n",
       "  'overpriced'],\n",
       " ['i',\n",
       "  'was',\n",
       "  'disgusted',\n",
       "  'because',\n",
       "  'i',\n",
       "  'was',\n",
       "  'pretty',\n",
       "  'sure',\n",
       "  'that',\n",
       "  'was',\n",
       "  'human',\n",
       "  'hair'],\n",
       " ['i', 'was', 'shocked', 'because', 'no', 'signs', 'indicate', 'cash', 'only'],\n",
       " ['waitress', 'was', 'a', 'little', 'slow', 'in', 'service'],\n",
       " ['did', 'not', 'like', 'at', 'all'],\n",
       " ['the', 'burrittos', 'blah'],\n",
       " ['the', 'worst', 'was', 'the', 'salmon', 'sashimi'],\n",
       " ['this', 'was', 'like', 'the', 'final', 'blow'],\n",
       " ['service', 'sucks'],\n",
       " ['the', 'turkey', 'and', 'roast', 'beef', 'were', 'bland'],\n",
       " ['the',\n",
       "  'pan',\n",
       "  'cakes',\n",
       "  'everyone',\n",
       "  'are',\n",
       "  'raving',\n",
       "  'about',\n",
       "  'taste',\n",
       "  'like',\n",
       "  'a',\n",
       "  'sugary',\n",
       "  'disaster',\n",
       "  'tailored',\n",
       "  'to',\n",
       "  'the',\n",
       "  'palate',\n",
       "  'of',\n",
       "  'a',\n",
       "  'six',\n",
       "  'year',\n",
       "  'old'],\n",
       " ['the',\n",
       "  'poor',\n",
       "  'batter',\n",
       "  'to',\n",
       "  'meat',\n",
       "  'ratio',\n",
       "  'made',\n",
       "  'the',\n",
       "  'chicken',\n",
       "  'tenders',\n",
       "  'very',\n",
       "  'unsatisfying'],\n",
       " ['say', 'bye', 'bye', 'to', 'your', 'tip', 'lady'],\n",
       " [\"we'll\", 'never', 'go', 'again'],\n",
       " ['wow...', 'loved', 'this', 'place'],\n",
       " ['stopped',\n",
       "  'by',\n",
       "  'during',\n",
       "  'the',\n",
       "  'late',\n",
       "  'may',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'off',\n",
       "  'rick',\n",
       "  'steve',\n",
       "  'recommendation',\n",
       "  'and',\n",
       "  'loved',\n",
       "  'it'],\n",
       " ['the',\n",
       "  'selection',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'was',\n",
       "  'great',\n",
       "  'and',\n",
       "  'so',\n",
       "  'were',\n",
       "  'the',\n",
       "  'prices'],\n",
       " ['the', 'fries', 'were', 'great', 'too'],\n",
       " ['a', 'great', 'touch'],\n",
       " ['service', 'was', 'very', 'prompt'],\n",
       " ['highly', 'recommended'],\n",
       " ['service', 'is', 'also', 'cute'],\n",
       " ['best', 'tacos', 'in', 'town', 'by', 'far'],\n",
       " ['so', 'they', 'performed'],\n",
       " [\"that's\",\n",
       "  'right....the',\n",
       "  'red',\n",
       "  'velvet',\n",
       "  'cake.....ohhh',\n",
       "  'this',\n",
       "  'stuff',\n",
       "  'is',\n",
       "  'so',\n",
       "  'good'],\n",
       " ['i',\n",
       "  'found',\n",
       "  'this',\n",
       "  'place',\n",
       "  'by',\n",
       "  'accident',\n",
       "  'and',\n",
       "  'i',\n",
       "  'could',\n",
       "  'not',\n",
       "  'be',\n",
       "  'happier'],\n",
       " ['each',\n",
       "  'day',\n",
       "  'of',\n",
       "  'the',\n",
       "  'week',\n",
       "  'they',\n",
       "  'have',\n",
       "  'a',\n",
       "  'different',\n",
       "  'deal',\n",
       "  'and',\n",
       "  \"it's\",\n",
       "  'all',\n",
       "  'so',\n",
       "  'delicious'],\n",
       " ['ample', 'portions', 'and', 'good', 'prices'],\n",
       " ['my', 'first', 'visit', 'to', 'hiro', 'was', 'a', 'delight'],\n",
       " ['their', 'chow', 'mein', 'is', 'so', 'good'],\n",
       " ['the', 'portion', 'was', 'huge'],\n",
       " ['this', 'place', 'receives', 'stars', 'for', 'their', 'appetizers'],\n",
       " ['the', 'cocktails', 'are', 'all', 'handmade', 'and', 'delicious'],\n",
       " ['my',\n",
       "  'drink',\n",
       "  'was',\n",
       "  'never',\n",
       "  'empty',\n",
       "  'and',\n",
       "  'he',\n",
       "  'made',\n",
       "  'some',\n",
       "  'really',\n",
       "  'great',\n",
       "  'menu',\n",
       "  'suggestions']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test!\n",
    "\n",
    "The following code is to check our data to ensure that it is counting correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 20, 1: 20}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 178, 1: 145}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wow': {0: 1},\n",
       " 'crust': {0: 1},\n",
       " 'is': {0: 1, 1: 3},\n",
       " 'not': {0: 4, 1: 1},\n",
       " 'good': {0: 1, 1: 3},\n",
       " 'tasty': {0: 1},\n",
       " 'and': {0: 4, 1: 7},\n",
       " 'the': {0: 12, 1: 8},\n",
       " 'texture': {0: 1},\n",
       " 'was': {0: 8, 1: 5},\n",
       " 'just': {0: 1},\n",
       " 'nasty': {0: 1},\n",
       " 'now': {0: 1},\n",
       " 'i': {0: 6, 1: 2},\n",
       " 'am': {0: 1},\n",
       " 'getting': {0: 1},\n",
       " 'angry': {0: 1},\n",
       " 'want': {0: 1},\n",
       " 'my': {0: 1, 1: 2},\n",
       " 'damn': {0: 1},\n",
       " 'pho': {0: 1},\n",
       " 'honeslty': {0: 1},\n",
       " 'it': {0: 2, 1: 1},\n",
       " \"didn't\": {0: 1},\n",
       " 'taste': {0: 2},\n",
       " 'that': {0: 2},\n",
       " 'fresh': {0: 1},\n",
       " 'potatoes': {0: 1},\n",
       " 'were': {0: 2, 1: 2},\n",
       " 'like': {0: 4},\n",
       " 'rubber': {0: 1},\n",
       " 'you': {0: 1},\n",
       " 'could': {0: 1, 1: 1},\n",
       " 'tell': {0: 1},\n",
       " 'they': {0: 1, 1: 2},\n",
       " 'had': {0: 3},\n",
       " 'been': {0: 1},\n",
       " 'made': {0: 2, 1: 1},\n",
       " 'up': {0: 2},\n",
       " 'ahead': {0: 1},\n",
       " 'of': {0: 2, 1: 1},\n",
       " 'time': {0: 1},\n",
       " 'being': {0: 2},\n",
       " 'kept': {0: 1},\n",
       " 'under': {0: 1},\n",
       " 'a': {0: 4, 1: 3},\n",
       " 'warmer': {0: 1},\n",
       " 'would': {0: 1},\n",
       " 'go': {0: 2},\n",
       " 'back': {0: 1},\n",
       " 'cashier': {0: 1},\n",
       " 'no': {0: 2},\n",
       " 'care': {0: 1},\n",
       " 'what': {0: 2},\n",
       " 'so': {0: 1, 1: 5},\n",
       " 'ever': {0: 1},\n",
       " 'on': {0: 1, 1: 1},\n",
       " 'to': {0: 4, 1: 1},\n",
       " 'say': {0: 2},\n",
       " 'still': {0: 1},\n",
       " 'ended': {0: 1},\n",
       " 'wayyy': {0: 1},\n",
       " 'overpriced': {0: 1},\n",
       " 'disgusted': {0: 1},\n",
       " 'because': {0: 2},\n",
       " 'pretty': {0: 1},\n",
       " 'sure': {0: 1},\n",
       " 'human': {0: 1},\n",
       " 'hair': {0: 1},\n",
       " 'shocked': {0: 1},\n",
       " 'signs': {0: 1},\n",
       " 'indicate': {0: 1},\n",
       " 'cash': {0: 1},\n",
       " 'only': {0: 1},\n",
       " 'waitress': {0: 1},\n",
       " 'little': {0: 1},\n",
       " 'slow': {0: 1},\n",
       " 'in': {0: 1, 1: 1},\n",
       " 'service': {0: 2, 1: 2},\n",
       " 'did': {0: 1},\n",
       " 'at': {0: 1},\n",
       " 'all': {0: 1, 1: 2},\n",
       " 'burrittos': {0: 1},\n",
       " 'blah': {0: 1},\n",
       " 'worst': {0: 1},\n",
       " 'salmon': {0: 1},\n",
       " 'sashimi': {0: 1},\n",
       " 'this': {0: 1, 1: 4},\n",
       " 'final': {0: 1},\n",
       " 'blow': {0: 1},\n",
       " 'sucks': {0: 1},\n",
       " 'turkey': {0: 1},\n",
       " 'roast': {0: 1},\n",
       " 'beef': {0: 1},\n",
       " 'bland': {0: 1},\n",
       " 'pan': {0: 1},\n",
       " 'cakes': {0: 1},\n",
       " 'everyone': {0: 1},\n",
       " 'are': {0: 1, 1: 1},\n",
       " 'raving': {0: 1},\n",
       " 'about': {0: 1},\n",
       " 'sugary': {0: 1},\n",
       " 'disaster': {0: 1},\n",
       " 'tailored': {0: 1},\n",
       " 'palate': {0: 1},\n",
       " 'six': {0: 1},\n",
       " 'year': {0: 1},\n",
       " 'old': {0: 1},\n",
       " 'poor': {0: 1},\n",
       " 'batter': {0: 1},\n",
       " 'meat': {0: 1},\n",
       " 'ratio': {0: 1},\n",
       " 'chicken': {0: 1},\n",
       " 'tenders': {0: 1},\n",
       " 'very': {0: 1, 1: 1},\n",
       " 'unsatisfying': {0: 1},\n",
       " 'bye': {0: 2},\n",
       " 'your': {0: 1},\n",
       " 'tip': {0: 1},\n",
       " 'lady': {0: 1},\n",
       " \"we'll\": {0: 1},\n",
       " 'never': {0: 1, 1: 1},\n",
       " 'again': {0: 1},\n",
       " 'wow...': {1: 1},\n",
       " 'loved': {1: 2},\n",
       " 'place': {1: 3},\n",
       " 'stopped': {1: 1},\n",
       " 'by': {1: 3},\n",
       " 'during': {1: 1},\n",
       " 'late': {1: 1},\n",
       " 'may': {1: 1},\n",
       " 'bank': {1: 1},\n",
       " 'holiday': {1: 1},\n",
       " 'off': {1: 1},\n",
       " 'rick': {1: 1},\n",
       " 'steve': {1: 1},\n",
       " 'recommendation': {1: 1},\n",
       " 'selection': {1: 1},\n",
       " 'menu': {1: 2},\n",
       " 'great': {1: 4},\n",
       " 'prices': {1: 2},\n",
       " 'fries': {1: 1},\n",
       " 'too': {1: 1},\n",
       " 'touch': {1: 1},\n",
       " 'prompt': {1: 1},\n",
       " 'highly': {1: 1},\n",
       " 'recommended': {1: 1},\n",
       " 'also': {1: 1},\n",
       " 'cute': {1: 1},\n",
       " 'best': {1: 1},\n",
       " 'tacos': {1: 1},\n",
       " 'town': {1: 1},\n",
       " 'far': {1: 1},\n",
       " 'performed': {1: 1},\n",
       " \"that's\": {1: 1},\n",
       " 'right....the': {1: 1},\n",
       " 'red': {1: 1},\n",
       " 'velvet': {1: 1},\n",
       " 'cake.....ohhh': {1: 1},\n",
       " 'stuff': {1: 1},\n",
       " 'found': {1: 1},\n",
       " 'accident': {1: 1},\n",
       " 'be': {1: 1},\n",
       " 'happier': {1: 1},\n",
       " 'each': {1: 1},\n",
       " 'day': {1: 1},\n",
       " 'week': {1: 1},\n",
       " 'have': {1: 1},\n",
       " 'different': {1: 1},\n",
       " 'deal': {1: 1},\n",
       " \"it's\": {1: 1},\n",
       " 'delicious': {1: 2},\n",
       " 'ample': {1: 1},\n",
       " 'portions': {1: 1},\n",
       " 'first': {1: 1},\n",
       " 'visit': {1: 1},\n",
       " 'hiro': {1: 1},\n",
       " 'delight': {1: 1},\n",
       " 'their': {1: 2},\n",
       " 'chow': {1: 1},\n",
       " 'mein': {1: 1},\n",
       " 'portion': {1: 1},\n",
       " 'huge': {1: 1},\n",
       " 'receives': {1: 1},\n",
       " 'stars': {1: 1},\n",
       " 'for': {1: 1},\n",
       " 'appetizers': {1: 1},\n",
       " 'cocktails': {1: 1},\n",
       " 'handmade': {1: 1},\n",
       " 'drink': {1: 1},\n",
       " 'empty': {1: 1},\n",
       " 'he': {1: 1},\n",
       " 'some': {1: 1},\n",
       " 'really': {1: 1},\n",
       " 'suggestions': {1: 1}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaPlace Smoothing\n",
    "\n",
    "Recall that LaPlace smoothing increases the count of words for each class by 1.\n",
    "How would you approach this problem?\n",
    "\n",
    "<details>\n",
    "  <summary>Click me to see my approach</summary>\n",
    "  \n",
    "  #### Simply modify the existing dictionary\n",
    "  For this step, I just simply iterate through the ```vocab_counts``` dictionary\n",
    "  and add a value of 1 for classes that do not have a particular word and\n",
    "  increment all other entries by one.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in vocab_counts.items():\n",
    "    for cl in class_counts.keys():\n",
    "        if cl not in v.keys():\n",
    "            v[cl] = 1\n",
    "        else:\n",
    "            v[cl] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wow': {0: 2, 1: 1},\n",
       " 'crust': {0: 2, 1: 1},\n",
       " 'is': {0: 2, 1: 4},\n",
       " 'not': {0: 5, 1: 2},\n",
       " 'good': {0: 2, 1: 4},\n",
       " 'tasty': {0: 2, 1: 1},\n",
       " 'and': {0: 5, 1: 8},\n",
       " 'the': {0: 13, 1: 9},\n",
       " 'texture': {0: 2, 1: 1},\n",
       " 'was': {0: 9, 1: 6},\n",
       " 'just': {0: 2, 1: 1},\n",
       " 'nasty': {0: 2, 1: 1},\n",
       " 'now': {0: 2, 1: 1},\n",
       " 'i': {0: 7, 1: 3},\n",
       " 'am': {0: 2, 1: 1},\n",
       " 'getting': {0: 2, 1: 1},\n",
       " 'angry': {0: 2, 1: 1},\n",
       " 'want': {0: 2, 1: 1},\n",
       " 'my': {0: 2, 1: 3},\n",
       " 'damn': {0: 2, 1: 1},\n",
       " 'pho': {0: 2, 1: 1},\n",
       " 'honeslty': {0: 2, 1: 1},\n",
       " 'it': {0: 3, 1: 2},\n",
       " \"didn't\": {0: 2, 1: 1},\n",
       " 'taste': {0: 3, 1: 1},\n",
       " 'that': {0: 3, 1: 1},\n",
       " 'fresh': {0: 2, 1: 1},\n",
       " 'potatoes': {0: 2, 1: 1},\n",
       " 'were': {0: 3, 1: 3},\n",
       " 'like': {0: 5, 1: 1},\n",
       " 'rubber': {0: 2, 1: 1},\n",
       " 'you': {0: 2, 1: 1},\n",
       " 'could': {0: 2, 1: 2},\n",
       " 'tell': {0: 2, 1: 1},\n",
       " 'they': {0: 2, 1: 3},\n",
       " 'had': {0: 4, 1: 1},\n",
       " 'been': {0: 2, 1: 1},\n",
       " 'made': {0: 3, 1: 2},\n",
       " 'up': {0: 3, 1: 1},\n",
       " 'ahead': {0: 2, 1: 1},\n",
       " 'of': {0: 3, 1: 2},\n",
       " 'time': {0: 2, 1: 1},\n",
       " 'being': {0: 3, 1: 1},\n",
       " 'kept': {0: 2, 1: 1},\n",
       " 'under': {0: 2, 1: 1},\n",
       " 'a': {0: 5, 1: 4},\n",
       " 'warmer': {0: 2, 1: 1},\n",
       " 'would': {0: 2, 1: 1},\n",
       " 'go': {0: 3, 1: 1},\n",
       " 'back': {0: 2, 1: 1},\n",
       " 'cashier': {0: 2, 1: 1},\n",
       " 'no': {0: 3, 1: 1},\n",
       " 'care': {0: 2, 1: 1},\n",
       " 'what': {0: 3, 1: 1},\n",
       " 'so': {0: 2, 1: 6},\n",
       " 'ever': {0: 2, 1: 1},\n",
       " 'on': {0: 2, 1: 2},\n",
       " 'to': {0: 5, 1: 2},\n",
       " 'say': {0: 3, 1: 1},\n",
       " 'still': {0: 2, 1: 1},\n",
       " 'ended': {0: 2, 1: 1},\n",
       " 'wayyy': {0: 2, 1: 1},\n",
       " 'overpriced': {0: 2, 1: 1},\n",
       " 'disgusted': {0: 2, 1: 1},\n",
       " 'because': {0: 3, 1: 1},\n",
       " 'pretty': {0: 2, 1: 1},\n",
       " 'sure': {0: 2, 1: 1},\n",
       " 'human': {0: 2, 1: 1},\n",
       " 'hair': {0: 2, 1: 1},\n",
       " 'shocked': {0: 2, 1: 1},\n",
       " 'signs': {0: 2, 1: 1},\n",
       " 'indicate': {0: 2, 1: 1},\n",
       " 'cash': {0: 2, 1: 1},\n",
       " 'only': {0: 2, 1: 1},\n",
       " 'waitress': {0: 2, 1: 1},\n",
       " 'little': {0: 2, 1: 1},\n",
       " 'slow': {0: 2, 1: 1},\n",
       " 'in': {0: 2, 1: 2},\n",
       " 'service': {0: 3, 1: 3},\n",
       " 'did': {0: 2, 1: 1},\n",
       " 'at': {0: 2, 1: 1},\n",
       " 'all': {0: 2, 1: 3},\n",
       " 'burrittos': {0: 2, 1: 1},\n",
       " 'blah': {0: 2, 1: 1},\n",
       " 'worst': {0: 2, 1: 1},\n",
       " 'salmon': {0: 2, 1: 1},\n",
       " 'sashimi': {0: 2, 1: 1},\n",
       " 'this': {0: 2, 1: 5},\n",
       " 'final': {0: 2, 1: 1},\n",
       " 'blow': {0: 2, 1: 1},\n",
       " 'sucks': {0: 2, 1: 1},\n",
       " 'turkey': {0: 2, 1: 1},\n",
       " 'roast': {0: 2, 1: 1},\n",
       " 'beef': {0: 2, 1: 1},\n",
       " 'bland': {0: 2, 1: 1},\n",
       " 'pan': {0: 2, 1: 1},\n",
       " 'cakes': {0: 2, 1: 1},\n",
       " 'everyone': {0: 2, 1: 1},\n",
       " 'are': {0: 2, 1: 2},\n",
       " 'raving': {0: 2, 1: 1},\n",
       " 'about': {0: 2, 1: 1},\n",
       " 'sugary': {0: 2, 1: 1},\n",
       " 'disaster': {0: 2, 1: 1},\n",
       " 'tailored': {0: 2, 1: 1},\n",
       " 'palate': {0: 2, 1: 1},\n",
       " 'six': {0: 2, 1: 1},\n",
       " 'year': {0: 2, 1: 1},\n",
       " 'old': {0: 2, 1: 1},\n",
       " 'poor': {0: 2, 1: 1},\n",
       " 'batter': {0: 2, 1: 1},\n",
       " 'meat': {0: 2, 1: 1},\n",
       " 'ratio': {0: 2, 1: 1},\n",
       " 'chicken': {0: 2, 1: 1},\n",
       " 'tenders': {0: 2, 1: 1},\n",
       " 'very': {0: 2, 1: 2},\n",
       " 'unsatisfying': {0: 2, 1: 1},\n",
       " 'bye': {0: 3, 1: 1},\n",
       " 'your': {0: 2, 1: 1},\n",
       " 'tip': {0: 2, 1: 1},\n",
       " 'lady': {0: 2, 1: 1},\n",
       " \"we'll\": {0: 2, 1: 1},\n",
       " 'never': {0: 2, 1: 2},\n",
       " 'again': {0: 2, 1: 1},\n",
       " 'wow...': {1: 2, 0: 1},\n",
       " 'loved': {1: 3, 0: 1},\n",
       " 'place': {1: 4, 0: 1},\n",
       " 'stopped': {1: 2, 0: 1},\n",
       " 'by': {1: 4, 0: 1},\n",
       " 'during': {1: 2, 0: 1},\n",
       " 'late': {1: 2, 0: 1},\n",
       " 'may': {1: 2, 0: 1},\n",
       " 'bank': {1: 2, 0: 1},\n",
       " 'holiday': {1: 2, 0: 1},\n",
       " 'off': {1: 2, 0: 1},\n",
       " 'rick': {1: 2, 0: 1},\n",
       " 'steve': {1: 2, 0: 1},\n",
       " 'recommendation': {1: 2, 0: 1},\n",
       " 'selection': {1: 2, 0: 1},\n",
       " 'menu': {1: 3, 0: 1},\n",
       " 'great': {1: 5, 0: 1},\n",
       " 'prices': {1: 3, 0: 1},\n",
       " 'fries': {1: 2, 0: 1},\n",
       " 'too': {1: 2, 0: 1},\n",
       " 'touch': {1: 2, 0: 1},\n",
       " 'prompt': {1: 2, 0: 1},\n",
       " 'highly': {1: 2, 0: 1},\n",
       " 'recommended': {1: 2, 0: 1},\n",
       " 'also': {1: 2, 0: 1},\n",
       " 'cute': {1: 2, 0: 1},\n",
       " 'best': {1: 2, 0: 1},\n",
       " 'tacos': {1: 2, 0: 1},\n",
       " 'town': {1: 2, 0: 1},\n",
       " 'far': {1: 2, 0: 1},\n",
       " 'performed': {1: 2, 0: 1},\n",
       " \"that's\": {1: 2, 0: 1},\n",
       " 'right....the': {1: 2, 0: 1},\n",
       " 'red': {1: 2, 0: 1},\n",
       " 'velvet': {1: 2, 0: 1},\n",
       " 'cake.....ohhh': {1: 2, 0: 1},\n",
       " 'stuff': {1: 2, 0: 1},\n",
       " 'found': {1: 2, 0: 1},\n",
       " 'accident': {1: 2, 0: 1},\n",
       " 'be': {1: 2, 0: 1},\n",
       " 'happier': {1: 2, 0: 1},\n",
       " 'each': {1: 2, 0: 1},\n",
       " 'day': {1: 2, 0: 1},\n",
       " 'week': {1: 2, 0: 1},\n",
       " 'have': {1: 2, 0: 1},\n",
       " 'different': {1: 2, 0: 1},\n",
       " 'deal': {1: 2, 0: 1},\n",
       " \"it's\": {1: 2, 0: 1},\n",
       " 'delicious': {1: 3, 0: 1},\n",
       " 'ample': {1: 2, 0: 1},\n",
       " 'portions': {1: 2, 0: 1},\n",
       " 'first': {1: 2, 0: 1},\n",
       " 'visit': {1: 2, 0: 1},\n",
       " 'hiro': {1: 2, 0: 1},\n",
       " 'delight': {1: 2, 0: 1},\n",
       " 'their': {1: 3, 0: 1},\n",
       " 'chow': {1: 2, 0: 1},\n",
       " 'mein': {1: 2, 0: 1},\n",
       " 'portion': {1: 2, 0: 1},\n",
       " 'huge': {1: 2, 0: 1},\n",
       " 'receives': {1: 2, 0: 1},\n",
       " 'stars': {1: 2, 0: 1},\n",
       " 'for': {1: 2, 0: 1},\n",
       " 'appetizers': {1: 2, 0: 1},\n",
       " 'cocktails': {1: 2, 0: 1},\n",
       " 'handmade': {1: 2, 0: 1},\n",
       " 'drink': {1: 2, 0: 1},\n",
       " 'empty': {1: 2, 0: 1},\n",
       " 'he': {1: 2, 0: 1},\n",
       " 'some': {1: 2, 0: 1},\n",
       " 'really': {1: 2, 0: 1},\n",
       " 'suggestions': {1: 2, 0: 1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_counts # numerator in MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the conditional probabilities data structure\n",
    "\n",
    "Up to now, we have only been getting counts of words, but now we need actual\n",
    "probabilities for our classifier to work.  Now that we have all the data we need\n",
    "to calculate these probabilities, how will you store this information (e.g. what\n",
    "would be an appropriate data structure)?\n",
    "\n",
    "<details>\n",
    "  <summary>Click me to see my approach</summary>\n",
    "  \n",
    "  #### Yet another dictionary!\n",
    "  For this case, I will use a dictionary with an identical format as our\n",
    "  ```vocab_counts``` dictionary, but instead of storing counts, I will be\n",
    "  storing the conditional probabilities.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary for the conditional probabilities\n",
    "cond_prob = {}\n",
    "\n",
    "# Iterate through our vocab_counts dictionary\n",
    "for k,v in vocab_counts.items():\n",
    "    # For each word, initialize a dictionary that will contain the probability\n",
    "    # of the word, given a certain class.\n",
    "    cond_prob[k] = {}\n",
    "    for cl in v.keys():\n",
    "        # The following is populated from the MLE that is seen at the beginning\n",
    "        # of this notebook.\n",
    "        cond_prob[k][cl] = vocab_counts[k][cl]/(class_word_counts[cl] + len(vocab_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wow': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'crust': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'is': {0: 0.005361930294906166, 1: 0.011764705882352941},\n",
       " 'not': {0: 0.013404825737265416, 1: 0.0058823529411764705},\n",
       " 'good': {0: 0.005361930294906166, 1: 0.011764705882352941},\n",
       " 'tasty': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'and': {0: 0.013404825737265416, 1: 0.023529411764705882},\n",
       " 'the': {0: 0.03485254691689008, 1: 0.026470588235294117},\n",
       " 'texture': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'was': {0: 0.024128686327077747, 1: 0.01764705882352941},\n",
       " 'just': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'nasty': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'now': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'i': {0: 0.01876675603217158, 1: 0.008823529411764706},\n",
       " 'am': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'getting': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'angry': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'want': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'my': {0: 0.005361930294906166, 1: 0.008823529411764706},\n",
       " 'damn': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'pho': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'honeslty': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'it': {0: 0.00804289544235925, 1: 0.0058823529411764705},\n",
       " \"didn't\": {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'taste': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'that': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'fresh': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'potatoes': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'were': {0: 0.00804289544235925, 1: 0.008823529411764706},\n",
       " 'like': {0: 0.013404825737265416, 1: 0.0029411764705882353},\n",
       " 'rubber': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'you': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'could': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'tell': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'they': {0: 0.005361930294906166, 1: 0.008823529411764706},\n",
       " 'had': {0: 0.010723860589812333, 1: 0.0029411764705882353},\n",
       " 'been': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'made': {0: 0.00804289544235925, 1: 0.0058823529411764705},\n",
       " 'up': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'ahead': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'of': {0: 0.00804289544235925, 1: 0.0058823529411764705},\n",
       " 'time': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'being': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'kept': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'under': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'a': {0: 0.013404825737265416, 1: 0.011764705882352941},\n",
       " 'warmer': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'would': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'go': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'back': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'cashier': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'no': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'care': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'what': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'so': {0: 0.005361930294906166, 1: 0.01764705882352941},\n",
       " 'ever': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'on': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'to': {0: 0.013404825737265416, 1: 0.0058823529411764705},\n",
       " 'say': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'still': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'ended': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'wayyy': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'overpriced': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'disgusted': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'because': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'pretty': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'sure': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'human': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'hair': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'shocked': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'signs': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'indicate': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'cash': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'only': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'waitress': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'little': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'slow': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'in': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'service': {0: 0.00804289544235925, 1: 0.008823529411764706},\n",
       " 'did': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'at': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'all': {0: 0.005361930294906166, 1: 0.008823529411764706},\n",
       " 'burrittos': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'blah': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'worst': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'salmon': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'sashimi': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'this': {0: 0.005361930294906166, 1: 0.014705882352941176},\n",
       " 'final': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'blow': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'sucks': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'turkey': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'roast': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'beef': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'bland': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'pan': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'cakes': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'everyone': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'are': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'raving': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'about': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'sugary': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'disaster': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'tailored': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'palate': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'six': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'year': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'old': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'poor': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'batter': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'meat': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'ratio': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'chicken': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'tenders': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'very': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'unsatisfying': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'bye': {0: 0.00804289544235925, 1: 0.0029411764705882353},\n",
       " 'your': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'tip': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'lady': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " \"we'll\": {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'never': {0: 0.005361930294906166, 1: 0.0058823529411764705},\n",
       " 'again': {0: 0.005361930294906166, 1: 0.0029411764705882353},\n",
       " 'wow...': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'loved': {1: 0.008823529411764706, 0: 0.002680965147453083},\n",
       " 'place': {1: 0.011764705882352941, 0: 0.002680965147453083},\n",
       " 'stopped': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'by': {1: 0.011764705882352941, 0: 0.002680965147453083},\n",
       " 'during': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'late': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'may': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'bank': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'holiday': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'off': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'rick': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'steve': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'recommendation': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'selection': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'menu': {1: 0.008823529411764706, 0: 0.002680965147453083},\n",
       " 'great': {1: 0.014705882352941176, 0: 0.002680965147453083},\n",
       " 'prices': {1: 0.008823529411764706, 0: 0.002680965147453083},\n",
       " 'fries': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'too': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'touch': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'prompt': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'highly': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'recommended': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'also': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'cute': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'best': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'tacos': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'town': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'far': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'performed': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " \"that's\": {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'right....the': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'red': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'velvet': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'cake.....ohhh': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'stuff': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'found': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'accident': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'be': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'happier': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'each': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'day': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'week': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'have': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'different': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'deal': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " \"it's\": {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'delicious': {1: 0.008823529411764706, 0: 0.002680965147453083},\n",
       " 'ample': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'portions': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'first': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'visit': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'hiro': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'delight': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'their': {1: 0.008823529411764706, 0: 0.002680965147453083},\n",
       " 'chow': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'mein': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'portion': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'huge': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'receives': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'stars': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'for': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'appetizers': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'cocktails': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'handmade': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'drink': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'empty': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'he': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'some': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'really': {1: 0.0058823529411764705, 0: 0.002680965147453083},\n",
       " 'suggestions': {1: 0.0058823529411764705, 0: 0.002680965147453083}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our text classifier\n",
    "\n",
    "Now that we have the conditional probabilities, we almost have a fully\n",
    "functioning text classifier.  We have a bit more math to implement for the\n",
    "classifier to work.  Recall from the slides what we need to do to predict the\n",
    "class from a given document.\n",
    "$$ C_{NB} = \\underset{c\\in C}{\\operatorname*{argmax}}\\ P(c_j)\\prod_{x\\in X}\n",
    "P(x|c) $$\n",
    "How will you approach this problem?\n",
    "\n",
    "<details>\n",
    "  <summary>Click me to see my approach</summary>\n",
    "  \n",
    "  #### Find the class that has the highest probability\n",
    "We have the dictionary that is associated with $P(x|c)$: ```cond_prob```.  We\n",
    "also can find $P(c_j)$ by using the data in ```class_counts``` as follows:\n",
    "```class_counts```/```sum(class_counts.values())```.\n",
    "\n",
    "Finally, we need to iterate through the document to get a count of all the words\n",
    "so that we can calculate the probability for each class and return the class\n",
    "with the greatest probability.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Server did a great job handling our large rowdy table.', 1),\n",
       " ('Would come back again if I had a sushi craving while in Vegas.', 1),\n",
       " ('He deserves 5 stars.', 1),\n",
       " ('My boyfriend and I came here for the first time on a recent trip to Vegas and could not have been more pleased with the quality of food and service.',\n",
       "  1),\n",
       " ('They have great dinners.', 1),\n",
       " ('Not my thing.', 0),\n",
       " (\"If you are reading this please don't go there.\", 0),\n",
       " ('Tonight I had the Elk Filet special...and it sucked.', 0),\n",
       " ('We ordered some old classics and some new dishes after going there a few times and were sorely disappointed with everything.',\n",
       "  0),\n",
       " ('A FLY was in my apple juice.. A FLY!!!!!!!!', 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test data from test.csv\n",
    "test_list = listify_csv('test.csv')\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tok_string):\n",
    "    cl_probabilities = {}\n",
    "    total_words_in_class = sum(class_counts.values())\n",
    "\n",
    "    for cl in class_counts.keys():\n",
    "        cl_probabilities[cl] = class_counts[cl]/total_words_in_class\n",
    "        for word in tok_string:\n",
    "            if word in cond_prob.keys():\n",
    "                cl_probabilities[cl] *= cond_prob[word][cl]\n",
    "\n",
    "    return max(cl_probabilities, key=cl_probabilities.get)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not', 'my', 'thing']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array = test_list[5][0].lower().strip('.,!\\n').split()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(doc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc620",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
